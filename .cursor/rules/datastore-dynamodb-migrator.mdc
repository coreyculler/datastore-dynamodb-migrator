---
alwaysApply: true
description: "DataStore to DynamoDB Migration Utility - Project Overview and Development Guide"
---

# DataStore to DynamoDB Migration Utility

## Project Overview
This utility migrates Google Cloud Platform (GCP) DataStore tables to Amazon Web Services (AWS) DynamoDB. The tool automatically converts each DataStore Kind into a separate DynamoDB table, handling schema differences and providing an interactive migration experience.

## Core Requirements

### Migration Logic
- **Source**: GCP DataStore tables
- **Target**: AWS DynamoDB tables  
- **Mapping**: Each DataStore Kind → Individual DynamoDB table
- **Auto-creation**: DynamoDB tables created automatically if they don't exist
- **Introspection**: Use reflection/introspection to avoid manual type definitions for each record

### Interactive Key Selection
- List all fields per Kind during migration
- Allow user to interactively select:
  - Primary key (partition key) for each Kind
  - Sort key (optional) for each Kind if needed
- Provide field type information to help with key selection decisions
  - Allow renaming of the DynamoDB partition key attribute while sourcing its value from the DataStore entity key (ID/Name)
  - Strip metadata fields not present on original records before writing (e.g., `__key__`, `__key_id__`, `__key_name__`)

### Language Preference
- **Primary Choice**: Golang (preferred for its strong concurrency, AWS SDK support, and introspection capabilities)

## Technical Architecture

### Key Components
1. **DataStore Reader**: Connect to GCP DataStore and enumerate Kinds/entities
2. **Schema Introspector**: Analyze field types and structures per Kind
3. **Interactive CLI**: Present field information and capture user key selections using `spf13/cobra` for command structure
4. **DynamoDB Manager**: Create tables with selected keys and handle data transformation
5. **Migration Engine**: Orchestrate the transfer process with progress tracking

### Development Guidelines
- Use introspection/reflection to handle arbitrary record structures
- Implement CLI using `spf13/cobra` for structured command handling and subcommands
- Implement robust error handling for network operations
- Provide clear progress indicators during migration
- Log all operations for debugging and audit purposes
- Handle large datasets efficiently (streaming/batching)
- Validate key selections before table creation

### Key Selection Details

- DataStore entity key identifier is exposed as an always-available field with display name "DataStore Primary Key (ID/Name)" and internal name `PK`.
- Users may rename the DynamoDB partition key attribute (alias) during the interactive flow. The tool will:
  - Continue sourcing the value from the DataStore key via `PK` (or the selected source field)
  - Store the alias as `PartitionKey` and the source as `PartitionKeySource` in the configuration
  - Infer the DynamoDB key attribute type from the source field while using the alias for the attribute name
- Before writing items to DynamoDB, the engine removes metadata/internal fields that weren't on the original source record, including `__key__`, `__key_name__`, `__key_id__`. If the PK is aliased, the synthetic `PK` field is also removed from the item payload.

### S3 Offloading (Large Records)

- For Kinds whose records may exceed DynamoDB's item size limits, provide an option to store full records in S3 as JSON.
- Interactive prompts per Kind should ask whether to enable S3 storage and, if enabled:
  - Ask for the S3 bucket (default from `MIGRATION_S3_BUCKET` if present)
  - Partition objects using the Kind name converted to kebab-case as the prefix (e.g., `UserActions` → `user-actions/`)
  - Name objects using the record's primary key with a `.json` suffix
  - Prompt to select which fields should still be written to DynamoDB as a minimal projection
  - Always add the full `s3://` path as `S3ObjectPath` in the DynamoDB item
  - Always include key attributes in DynamoDB regardless of projection

### Expected File Structure
```
├── main.go                 # Entry point and CLI setup
├── internal/
│   ├── datastore/         # GCP DataStore operations
│   ├── dynamodb/          # AWS DynamoDB operations  
│   ├── migration/         # Core migration logic
│   ├── introspection/     # Schema analysis utilities
│   └── cli/               # Interactive user interface
├── config/                # Configuration management
├── go.mod                 # Go module dependencies
└── README.md              # Project documentation
```

## Development Priorities
1. **Reliability**: Handle network failures, partial migrations gracefully
2. **User Experience**: Clear prompts, progress indicators, helpful error messages
3. **Performance**: Efficient handling of large datasets
4. **Flexibility**: Support various DataStore schema patterns
5. **Safety**: Validate operations before execution, provide dry-run capability

## Key Dependencies (Golang)
- `cloud.google.com/go/datastore` - GCP DataStore client
- `github.com/aws/aws-sdk-go-v2` - AWS DynamoDB client  
- `github.com/spf13/cobra` - CLI framework
- `github.com/manifoldco/promptui` - Interactive prompts
- Standard library `reflect` package for introspection
